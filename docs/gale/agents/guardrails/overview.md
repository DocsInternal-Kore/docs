# Guardrails

Guardrails are safety measures that provide guidelines and limits to ensure that the responses of the ML models are aligned with ethical standards and societal expectations. You can deploy various guardrail models and use them to scan the inputs or prompts and output results. 

For example, deploying a "Toxicity" scanner involves analyzing and assessing the toxicity of a prompt, thereby safeguarding the health and safety of online interactions.